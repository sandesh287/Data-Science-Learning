# Advanced transformers: BERT Variants and GPT-3
# Experiment with a BERT variant (eg. RoBERTa) and fine-tune it on an NLP task.

# Since the dataset is too big, it requires more resources, which cannot be handled by our personal computer. It is going to take around 8-10 hours to train the model. I have tried training the model, but my computer only handles for 1% training.



# libraries
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset


# Load dataset
dataset = load_dataset('ag_news')


# Load RoBERTa tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('roberta-base')
model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=4)


# Tokenize dataset
def tokenize_function(examples):
  return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)


# Prepare dataset
tokenized_datasets = tokenized_datasets.remove_columns(['text'])
tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')
tokenized_datasets.set_format('torch')


# Get the train and test dataset
train_dataset = tokenized_datasets['train']
test_dataset = tokenized_datasets['test']


# Training arguments
training_args = TrainingArguments(
  output_dir='./results',
  evaluation_strategy='epoch',
  learning_rate=2e-5,
  per_device_train_batch_size=16,
  per_device_eval_batch_size=16,
  num_train_epochs=3,
  weight_decay=0.01,
  save_steps=500
)


# Trainer
trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=test_dataset,
  tokenizer=tokenizer
)


# Train model
trainer.train()


# Evaluate model
results = trainer.evaluate()

print(f'Evaluation Results: {results}')